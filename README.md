# Opencode OpenAI-Compatible API

OpenAI API ν•μ‹μ„ μ‚¬μ©ν•μ—¬ Gemini λ¨λΈμ„ νΈμ¶ν•  μ μλ” νΈν™ λ μ΄μ–΄μ…λ‹λ‹¤.

## π“‹ λ©μ°¨

- [κ°μ”](#κ°μ”)
- [μ£Όμ” κΈ°λ¥](#μ£Όμ”-κΈ°λ¥)
- [μ„¤μΉ λ° μ‹¤ν–‰](#μ„¤μΉ-λ°-μ‹¤ν–‰)
- [API μ—”λ“ν¬μΈνΈ](#api-μ—”λ“ν¬μΈνΈ)
- [μƒμ„Έ API λ¬Έμ„](#μƒμ„Έ-api-λ¬Έμ„)

## π― κ°μ”

μ΄ ν”„λ΅μ νΈλ” OpenAI Chat Completions APIμ™€ νΈν™λλ” μΈν„°νμ΄μ¤λ¥Ό μ κ³µν•μ—¬ Gemini λ¨λΈμ„ μ‚¬μ©ν•  μ μλ„λ΅ ν•©λ‹λ‹¤. LiteLLMμ„ ν™μ©ν•μ—¬ λ¨λΈ νΈμ¶μ„ μ¤‘κ³„ν•©λ‹λ‹¤.

## β¨ μ£Όμ” κΈ°λ¥

### 1. OpenAI νΈν™ API
- `/chat/completions` λ° `/v1/chat/completions` μ—”λ“ν¬μΈνΈ μ κ³µ
- OpenAI SDKμ™€ μ™„λ²½ν•κ² νΈν™λλ” μ”μ²­/μ‘λ‹µ ν•μ‹

### 2. μ¤νΈλ¦¬λ° μ§€μ›
- Server-Sent Events (SSE)λ¥Ό ν†µν• μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° μ‘λ‹µ
- `stream: true` νλΌλ―Έν„°λ΅ ν™μ„±ν™”

### 3. Tool Calls (Function Calling) μ§€μ›
- OpenAI Function Callingκ³Ό λ™μΌν• λ°©μ‹μΌλ΅ λ„κµ¬ νΈμ¶ κ°€λ¥
- `tools` λ° `tool_choice` νλΌλ―Έν„° μ§€μ›

### 4. λ¨λΈ λ§¤ν•‘
- μ‚¬μ©μ μ •μ λ¨λΈλ…μ„ μ‹¤μ  Gemini λ¨λΈλ΅ μλ™ λ§¤ν•‘
- μ„¤μ • νμΌ(`opencode.json`)μ„ ν†µν• μ μ—°ν• λ¨λΈ κ΄€λ¦¬

### 5. μƒμ„Έν• λ΅κΉ…
- μ”μ²­/μ‘λ‹µ λ‚΄μ©μ μƒμ„Έν• λ΅κΉ…
- λ””λ²„κΉ…μ„ μ„ν• κ° μ²­ν¬(chunk) λ‹¨μ„ μ¶λ ¥

## π€ μ„¤μΉ λ° μ‹¤ν–‰

### ν•„μ μ”κµ¬μ‚¬ν•­
- Python 3.8+
- Gemini API ν‚¤

### ν™κ²½ λ³€μ μ„¤μ •

```bash
export GEMINI_API_KEY="your-gemini-api-key"
export GEMINI_API_BASE="https://your-gemini-endpoint.com/v1"
```

### μ‹¤ν–‰

```bash
# κ°λ° λ¨λ“
python app.py

# λλ” run.sh μ‚¬μ©
./run.sh
```

## π“΅ API μ—”λ“ν¬μΈνΈ

### 1. Health Check
```
GET /health
```

μ„λ²„ μƒνƒ ν™•μΈ

### 2. Chat Completions
```
POST /chat/completions
POST /v1/chat/completions
```

OpenAI Chat Completions APIμ™€ νΈν™λλ” μ±„ν… μ™„μ„± μ—”λ“ν¬μΈνΈ

**μ£Όμ” νλΌλ―Έν„°:**
- `model` (string, required): μ‚¬μ©ν•  λ¨λΈλ…
- `messages` (array, required): λ€ν™” λ©”μ‹μ§€ λ°°μ—΄
- `temperature` (float, optional): μ‘λ‹µμ λ¬΄μ‘μ„μ„± (κΈ°λ³Έκ°’: 0.7)
- `max_tokens` (integer, optional): μµλ€ ν† ν° μ (κΈ°λ³Έκ°’: 1000)
- `stream` (boolean, optional): μ¤νΈλ¦¬λ° λ¨λ“ ν™μ„±ν™” (κΈ°λ³Έκ°’: false)
- `tools` (array, optional): Function callingμ© λ„κµ¬ μ •μ
- `tool_choice` (string/object, optional): λ„κµ¬ μ„ νƒ λ°©μ‹ (κΈ°λ³Έκ°’: "auto")

## π“ μƒμ„Έ λ¬Έμ„

μƒμ„Έν• μ”μ²­/μ‘λ‹µ ν•μ‹κ³Ό μμ λ” λ‹¤μ λ¬Έμ„λ¥Ό μ°Έκ³ ν•μ„Έμ”:

- [API λ…μ„Έμ„ (API_SPEC.md)](./API_SPEC.md) - OpenAI νΈν™ API μƒμ„Έ μ¤ν™
- [μ‹¤μ  μμ  (EXAMPLES.md)](./EXAMPLES.md) - μ‹¤μ  Request/Response λ΅κ·Έ κΈ°λ° μμ 

## β™οΈ μ„¤μ • νμΌ (opencode.json)

`opencode.json` νμΌμ„ ν†µν•΄ ν”„λ΅λ°”μ΄λ”μ™€ λ¨λΈμ„ μ„¤μ •ν•  μ μμµλ‹λ‹¤.

### μ„¤μ • μμ 

```json
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "KTDS Model": {
      "api": "https://generativelanguage.googleapis.com",
      "options": {
        "baseURL": "https://generativelanguage.googleapis.com/v1beta/openai/",
        "apiKey": "your-api-key-here"
      },
      "models": {
        "gemini-3-flash-preview": {
          "name": "Gemini 3 Flash"
        }
      }
    },
    "Local Provider": {
      "api": "http://localhost:8000",
      "options": {
        "baseURL": "http://localhost:8000"
      },
      "models": {
        "local-model": {
          "name": "Local Model"
        }
      }
    }
  }
}
```

### μ„¤μ • κµ¬μ΅°

- **provider**: ν”„λ΅λ°”μ΄λ” λ©λ΅μ„ μ •μ
  - **api**: API μ—”λ“ν¬μΈνΈ URL
  - **options**: μ¶”κ°€ μµμ…
    - **baseURL**: μ‹¤μ  νΈμ¶λ  Base URL
    - **apiKey**: API ν‚¤ (μ„ νƒμ‚¬ν•­)
  - **models**: μ‚¬μ© κ°€λ¥ν• λ¨λΈ μ •μ
    - **λ¨λΈ ID**: λ‚΄λ¶€μ μΌλ΅ μ‚¬μ©ν•  λ¨λΈ μ‹λ³„μ
    - **name**: ν‘μ‹λ  λ¨λΈ μ΄λ¦„

## π”§ μ•„ν‚¤ν…μ²

### μ£Όμ” μ»΄ν¬λ„νΈ

1. **app/routes/chat.py**: Chat Completions API μ—”λ“ν¬μΈνΈ κµ¬ν„
   - μ”μ²­ νλΌλ―Έν„° μ¶”μ¶ λ° κ²€μ¦
   - λ¨λΈλ… ν•΄μ„ (resolve_model)
   - LiteLLMμ„ ν†µν• Gemini API νΈμ¶
   - μ¤νΈλ¦¬λ°/λΉ„μ¤νΈλ¦¬λ° μ‘λ‹µ μ²λ¦¬
   - Tool calls μ²λ¦¬

2. **app/utils/model_resolver.py**: λ¨λΈλ… λ§¤ν•‘ λ΅μ§
   - μ‚¬μ©μ μ •μ λ¨λΈλ… β†’ μ‹¤μ  Gemini λ¨λΈλ… λ³€ν™

3. **app/utils/formatter.py**: μ‘λ‹µ ν¬λ§·ν…
   - LiteLLM μ‘λ‹µμ„ OpenAI ν•μ‹μΌλ΅ λ³€ν™
   - μ¤νΈλ¦¬λ° μ²­ν¬ ν¬λ§·ν…

4. **app/config.py**: μ• ν”λ¦¬μΌ€μ΄μ… μ„¤μ •
   - ν™κ²½ λ³€μ κ΄€λ¦¬
   - λ¨λΈ λ§¤ν•‘ μ„¤μ • λ΅λ“

## π“ νΈμ¶ νλ¦„

```
Client Request (OpenAI format)
    β†“
Chat Blueprint (/chat/completions)
    β†“
Request Validation & Parameter Extraction
    β†“
Model Name Resolution (model_resolver)
    β†“
LiteLLM Completion Call (Gemini API)
    β†“
Response Formatting (formatter)
    β†“
Client Response (OpenAI format)
```

### μƒμ„Έ μ²λ¦¬ λ‹¨κ³„

1. **μ”μ²­ μ ‘μ**: Flask Blueprintμ—μ„ POST μ”μ²­ μμ‹ 
2. **νλΌλ―Έν„° μ¶”μ¶**: `model`, `messages`, `temperature`, `max_tokens`, `stream`, `tools` λ“± μ¶”μ¶
3. **λ¨λΈ λ§¤ν•‘**: `resolve_model()` ν•¨μλ΅ μ‚¬μ©μ λ¨λΈλ…μ„ μ‹¤μ  Gemini λ¨λΈλ΅ λ³€ν™
   - μ: `"local-model"` β†’ `"openai/gemini-2.5-flash"`
4. **API νΈμ¶**: LiteLLMμ `completion()` ν•¨μλ΅ Gemini API νΈμ¶
5. **μ‘λ‹µ μ²λ¦¬**:
   - λΉ„μ¤νΈλ¦¬λ°: μ „μ²΄ μ‘λ‹µμ„ JSONμΌλ΅ λ°ν™
   - μ¤νΈλ¦¬λ°: SSE ν•μ‹μΌλ΅ μ²­ν¬ λ‹¨μ„ μ¤νΈλ¦¬λ°
6. **ν¬λ§·ν…**: OpenAI μ‘λ‹µ ν•μ‹μΌλ΅ λ³€ν™ν•μ—¬ λ°ν™
   - λ¨λΈλ…μ„ μ‚¬μ©μκ°€ μ”μ²­ν• μ΄λ¦„μΌλ΅ λλλ¦Ό

### μ‹¤μ  μ²λ¦¬ μμ 

```
π“¥ Client Request
   model: "local-model"
   
   β†“

π”„ Model Resolution
   "local-model" β†’ "openai/gemini-2.5-flash"
   
   β†“

π”µ LiteLLM Call
   model: "openai/gemini-2.5-flash"
   api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
   
   β†“

π Gemini API Response
   model: "gemini-2.5-flash"
   content: "μ €λ” κµ¬κΈ€μ—μ„ ν›λ ¨ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ..."
   
   β†“

π“¤ Client Response
   model: "local-model" (μ›λ μ”μ²­ λ¨λΈλ…μΌλ΅ λ³µμ›)
   content: "μ €λ” κµ¬κΈ€μ—μ„ ν›λ ¨ν• λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ..."
```

## π” λ΅κΉ… κΈ°λ¥

κ°λ° λ° λ””λ²„κΉ…μ„ μ„ν•΄ μƒμ„Έν• λ΅κΉ…μ„ μ κ³µν•©λ‹λ‹¤:

- β… μ”μ²­ λ³Έλ¬Έ (κΈ΄ λ‚΄μ©μ€ μλ™ μ”μ•½)
- β… LiteLLM νλΌλ―Έν„°
- β… API ν‚¤ μ •λ³΄ (λ§μ¤ν‚Ή λ° μ „μ²΄)
- β… μ¤νΈλ¦¬λ° μ²­ν¬ λ‹¨μ„ μ¶λ ¥
- β… μµμΆ… μ‘λ‹µ λ‚΄μ©
- β… μ—λ¬ λ°μƒ μ‹ μƒμ„Έ μ¤νƒ μ¶”μ 

### λ΅κ·Έ μ¶λ ¥ μμ 

```
================================================================================
π€ ENDPOINT CALLED: /chat/completions
================================================================================

π“¥ REQUEST BODY (μ”μ•½):
{
  "model": "local-model",
  "messages": [...],
  "tools": [...],
  "stream": true
}

π”§ Tools: 12 tools (μ²« λ²μ§Έλ§ μ „μ²΄ ν‘μ‹, λ‚λ¨Έμ§€λ” nameλ§)
  [0] question (μ „μ²΄ ν‘μ‹λ¨)
  [1] bash
  [2] read
  ...

================================================================================
π”µ LITELLM PARAMS (μ”μ•½):
================================================================================
{
  "model": "openai/gemini-2.5-flash",
  "temperature": 0.7,
  "max_tokens": 1000,
  "api_base": "https://generativelanguage.googleapis.com/v1beta/openai/",
  "stream": true
}

π”‘ FULL API KEY: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8
π”‘ API KEY from env: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8
π”‘ API KEY from config: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8

π”µ Starting streaming response...

π“¦ Chunk #1 (RAW):
{
  "id": "ZQt6aeD2G5Oe0-kP9dOBgAE",
  "model": "gemini-2.5-flash",
  "choices": [...]
}

π“¤ Formatted Chunk #1:
data: {"id": "...", "model": "local-model", ...}

β… Streaming completed: 2 total chunks
```

### λ΅κ·Έ νΉμ§•

- **κΈ΄ λ¬Έμμ—΄ μλ™ μ”μ•½**: 10,000μ μ΄μƒμ κΈ΄ contentλ” "... (μ΄ Nμ)" ν•νƒλ΅ μ”μ•½
- **Tools μ”μ•½**: μ²« λ²μ§Έ toolλ§ μ „μ²΄ ν‘μ‹, λ‚λ¨Έμ§€λ” μ΄λ¦„λ§ ν‘μ‹
- **API ν‚¤ λ§μ¤ν‚Ή**: μ”μ•½λ³Έμ—μ„λ” λ§μ¤ν‚Ή, λ””λ²„κΉ…μ©μΌλ΅ μ „μ²΄ ν‚¤λ„ μ¶λ ¥
- **μ²­ν¬ λ‹¨μ„ μ¶λ ¥**: μ¤νΈλ¦¬λ° λ¨λ“μ—μ„ κ° μ²­ν¬λ¥Ό RAW λ° Formatted ν•νƒλ΅ μ¶λ ¥
- **μ‹¤μ‹κ°„ ν”λ¬μ‹**: λ¨λ“  λ΅κ·Έλ” `flush=True`λ΅ μ¦‰μ‹ μ¶λ ¥

## π› οΈ κ°λ°

### ν…μ¤νΈ

```bash
# λΉ λ¥Έ ν…μ¤νΈ
./quick_test.sh

# Python ν…μ¤νΈ μ¤ν¬λ¦½νΈ
python test_api.py

# cURL ν…μ¤νΈ
./test_chat_completions.sh
```

## π“„ λΌμ΄μ„Όμ¤

ν”„λ΅μ νΈ λΌμ΄μ„Όμ¤ μ •λ³΄λ¥Ό μ¶”κ°€ν•μ„Έμ”.