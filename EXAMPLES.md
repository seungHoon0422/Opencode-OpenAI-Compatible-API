# ì‹¤ì œ Request/Response ì˜ˆì œ

ì´ ë¬¸ì„œëŠ” ì‹¤ì œ API í˜¸ì¶œ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ëœ Request/Response ì˜ˆì œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì„¤ì • íŒŒì¼ (opencode.json)](#ì„¤ì •-íŒŒì¼-opencodejson)
- [ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­/ì‘ë‹µ ì˜ˆì œ](#ìŠ¤íŠ¸ë¦¬ë°-ìš”ì²­ì‘ë‹µ-ì˜ˆì œ)
- [Tool Calls í¬í•¨ ìš”ì²­](#tool-calls-í¬í•¨-ìš”ì²­)
- [ëª¨ë¸ ë§¤í•‘ ì˜ˆì œ](#ëª¨ë¸-ë§¤í•‘-ì˜ˆì œ)
- [ë¡œê·¸ ë¶„ì„](#ë¡œê·¸-ë¶„ì„)

---

## âš™ï¸ ì„¤ì • íŒŒì¼ (opencode.json)

### íŒŒì¼ êµ¬ì¡°

```json
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "KTDS Model": {
      "api": "https://generativelanguage.googleapis.com",
      "options": {
        "baseURL": "https://generativelanguage.googleapis.com/v1beta/openai/",
        "apiKey": "your-api-key-here"
      },
      "models": {
        "gemini-3-flash-preview": {
          "name": "Gemini 3 Flash"
        }
      }
    },
    "Local Provider": {
      "api": "http://localhost:8000",
      "options": {
        "baseURL": "http://localhost:8000"
      },
      "models": {
        "local-model": {
          "name": "Local Model"
        }
      }
    }
  }
}
```

### ì„¤ì • ì„¤ëª…

| í•„ë“œ | ì„¤ëª… |
|------|------|
| `$schema` | ì„¤ì • ìŠ¤í‚¤ë§ˆ URL |
| `provider` | í”„ë¡œë°”ì´ë” ëª©ë¡ |
| `provider.{name}.api` | API ì—”ë“œí¬ì¸íŠ¸ |
| `provider.{name}.options.baseURL` | ì‹¤ì œ í˜¸ì¶œë  Base URL |
| `provider.{name}.options.apiKey` | API í‚¤ (ì„ íƒì‚¬í•­) |
| `provider.{name}.models` | ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ |

---

## ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­/ì‘ë‹µ ì˜ˆì œ

### Request

**ì—”ë“œí¬ì¸íŠ¸:** `POST /chat/completions`

**Headers:**
```
Content-Type: application/json
```

**Body:**
```json
{
  "model": "local-model",
  "messages": [
    {
      "role": "system",
      "content": "You are opencode, an interactive CLI tool that helps users with software engineering tasks."
    },
    {
      "role": "user",
      "content": "ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"
    }
  ],
  "stream": true,
  "stream_options": {
    "include_usage": true
  }
}
```

### LiteLLM ë³€í™˜ íŒŒë¼ë¯¸í„°

ìš”ì²­ì´ LiteLLMìœ¼ë¡œ ì „ë‹¬ë  ë•Œ ë‹¤ìŒê³¼ ê°™ì´ ë³€í™˜ë©ë‹ˆë‹¤:

```json
{
  "model": "openai/gemini-2.5-flash",
  "messages": [
    {
      "role": "system",
      "content": "You are opencode, an interactive CLI tool that helps users with software engineering tasks."
    },
    {
      "role": "user",
      "content": "ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"
    }
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "api_key": "AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8",
  "api_base": "https://generativelanguage.googleapis.com/v1beta/openai/",
  "stream": true,
  "custom_llm_provider": "openai"
}
```

### Response Stream

**Status Code:** `200 OK`

**Content-Type:** `text/event-stream`

#### Chunk #1

```json
data: {
  "id": "ZQt6aeD2G5Oe0-kP9dOBgAE",
  "object": "chat.completion.chunk",
  "created": 1769605989,
  "model": "local-model",
  "choices": [
    {
      "index": 0,
      "delta": {
        "role": "assistant",
        "content": "ì €ëŠ” êµ¬ê¸€ì—ì„œ í›ˆë ¨í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ë©°, ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…ì„ ì§€ì›í•©ë‹ˆë‹¤."
      },
      "finish_reason": null
    }
  ]
}
```

#### Chunk #2 (Final)

```json
data: {
  "id": "ZQt6aeD2G5Oe0-kP9dOBgAE",
  "object": "chat.completion.chunk",
  "created": 1769605989,
  "model": "local-model",
  "choices": [
    {
      "index": 0,
      "delta": {},
      "finish_reason": "stop"
    }
  ]
}
```

#### Stream End

```
data: [DONE]
```

### ì‘ë‹µ ë¶„ì„

- **ì´ ì²­í¬ ìˆ˜**: 2ê°œ
- **ì²« ë²ˆì§¸ ì²­í¬**: `role`ê³¼ `content` í¬í•¨
- **ë§ˆì§€ë§‰ ì²­í¬**: ë¹ˆ `delta`ì™€ `finish_reason: "stop"` í¬í•¨
- **ì‘ë‹µ ì‹œê°„**: ì•½ 1ì´ˆ

---

## ğŸ› ï¸ Tool Calls í¬í•¨ ìš”ì²­

### Request with Tools

```json
{
  "model": "local-model",
  "messages": [
    {
      "role": "system",
      "content": "You are opencode, an interactive CLI tool that helps users with software engineering tasks."
    },
    {
      "role": "user",
      "content": "ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "question",
        "description": "Use this tool when you need to ask the user questions during execution.",
        "parameters": {
          "$schema": "https://json-schema.org/draft/2020-12/schema",
          "type": "object",
          "properties": {
            "questions": {
              "description": "Questions to ask",
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "question": {
                    "description": "Complete question",
                    "type": "string"
                  },
                  "header": {
                    "description": "Very short label (max 30 chars)",
                    "type": "string"
                  },
                  "options": {
                    "description": "Available choices",
                    "type": "array",
                    "items": {
                      "type": "object",
                      "properties": {
                        "label": {
                          "description": "Display text (1-5 words, concise)",
                          "type": "string"
                        },
                        "description": {
                          "description": "Explanation of choice",
                          "type": "string"
                        }
                      },
                      "required": ["label", "description"]
                    }
                  },
                  "multiple": {
                    "description": "Allow selecting multiple choices",
                    "type": "boolean"
                  }
                },
                "required": ["question", "header", "options"]
              }
            }
          },
          "required": ["questions"]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "bash"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "read"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "glob"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "grep"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "edit"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "write"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "task"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "webfetch"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "todowrite"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "todoread"
      }
    },
    {
      "type": "function",
      "function": {
        "name": "skill"
      }
    }
  ],
  "tool_choice": "auto",
  "stream": true,
  "stream_options": {
    "include_usage": true
  }
}
```

### Tools ëª©ë¡

ì´ ìš”ì²­ì—ëŠ” **12ê°œì˜ ë„êµ¬**ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

| # | Tool Name | ì„¤ëª… |
|---|-----------|------|
| 0 | `question` | ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•˜ê¸° (ë³µì¡í•œ íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ í¬í•¨) |
| 1 | `bash` | ì‰˜ ëª…ë ¹ ì‹¤í–‰ |
| 2 | `read` | íŒŒì¼ ì½ê¸° |
| 3 | `glob` | íŒŒì¼ ê²€ìƒ‰ (glob íŒ¨í„´) |
| 4 | `grep` | í…ìŠ¤íŠ¸ ê²€ìƒ‰ |
| 5 | `edit` | íŒŒì¼ í¸ì§‘ |
| 6 | `write` | íŒŒì¼ ì“°ê¸° |
| 7 | `task` | ì‘ì—… ê´€ë¦¬ |
| 8 | `webfetch` | ì›¹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° |
| 9 | `todowrite` | TODO ì‘ì„± |
| 10 | `todoread` | TODO ì½ê¸° |
| 11 | `skill` | ìŠ¤í‚¬ ê´€ë¦¬ |

### ë¡œê·¸ ì¶œë ¥

```
ğŸ”§ Tools: 12 tools (ì²« ë²ˆì§¸ë§Œ ì „ì²´ í‘œì‹œ, ë‚˜ë¨¸ì§€ëŠ” nameë§Œ)
  [0] question (ì „ì²´ í‘œì‹œë¨)
  [1] bash
  [2] read
  [3] glob
  [4] grep
  [5] edit
  [6] write
  [7] task
  [8] webfetch
  [9] todowrite
  [10] todoread
  [11] skill
```

---

## ğŸ”„ ëª¨ë¸ ë§¤í•‘ ì˜ˆì œ

### ë§¤í•‘ íë¦„

```
Client Request                LiteLLM Params               Gemini API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ model:          â”‚          â”‚ model:               â”‚     â”‚                 â”‚
â”‚ "local-model"   â”‚  â”€â”€â”€â”€â”€>  â”‚ "openai/gemini-2.5-  â”‚ â”€â”€> â”‚ Gemini API Call â”‚
â”‚                 â”‚          â”‚  flash"              â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ìƒì„¸ ë§¤í•‘ ì •ë³´

1. **í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ëª¨ë¸**: `"local-model"`
2. **í•´ì„ëœ ëª¨ë¸**: `"openai/gemini-2.5-flash"`
3. **í”„ë¡œë°”ì´ë”**: `"openai"` (custom_llm_provider)
4. **API Base**: `"https://generativelanguage.googleapis.com/v1beta/openai/"`

### ë§¤í•‘ ë¡œì§

```python
# app/utils/model_resolver.py
def resolve_model(user_model, model_mapping):
    """
    ì‚¬ìš©ì ëª¨ë¸ëª…ì„ ì‹¤ì œ LiteLLM ëª¨ë¸ëª…ìœ¼ë¡œ ë³€í™˜
    
    ì˜ˆ:
    - "local-model" -> "openai/gemini-2.5-flash"
    - "gpt-4" -> "gemini-1.5-pro"
    """
    return model_mapping.get(user_model, "default-model")
```

---

## ğŸ“Š ë¡œê·¸ ë¶„ì„

### Request ì²˜ë¦¬ ê³¼ì •

```
================================================================================
ğŸš€ ENDPOINT CALLED: /chat/completions
================================================================================

ğŸ“¥ REQUEST BODY (ìš”ì•½):
  - model: "local-model"
  - messages: 2ê°œ (system, user)
  - tools: 12ê°œ
  - stream: true

ğŸ”§ Tools: 12 tools (ì²« ë²ˆì§¸ë§Œ ì „ì²´ í‘œì‹œ, ë‚˜ë¨¸ì§€ëŠ” nameë§Œ)

================================================================================
ğŸ”µ LITELLM PARAMS (ìš”ì•½):
================================================================================
  - model: "openai/gemini-2.5-flash"
  - temperature: 0.7
  - max_tokens: 1000
  - api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"
  - stream: true

ğŸ”‘ FULL API KEY: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8
ğŸ”‘ API KEY from env: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8
ğŸ”‘ API KEY from config: AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8

ğŸ”µ Starting streaming response...

ğŸ“¦ Chunk #1 (RAW):
  - id: "ZQt6aeD2G5Oe0-kP9dOBgAE"
  - model: "gemini-2.5-flash"
  - role: "assistant"
  - content: "ì €ëŠ” êµ¬ê¸€ì—ì„œ í›ˆë ¨í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ë©°, ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…ì„ ì§€ì›í•©ë‹ˆë‹¤."

ğŸ“¤ Formatted Chunk #1:
  - model ë³€í™˜: "gemini-2.5-flash" -> "local-model"
  - content ì¸ì½”ë”©: UTF-8 ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜

ğŸ“¦ Chunk #2 (RAW):
  - finish_reason: "stop"
  - delta: {} (ë¹ˆ ê°ì²´)

âœ… Streaming completed: 2 total chunks
```

### ì²˜ë¦¬ ì‹œê°„

| ë‹¨ê³„ | ì‹œê°„ |
|------|------|
| Request ìˆ˜ì‹  | 22:13:08 |
| LiteLLM í˜¸ì¶œ | 22:13:08 |
| ì²« ë²ˆì§¸ ì²­í¬ | 22:13:09 |
| ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ | 22:13:09 |
| **ì´ ì†Œìš” ì‹œê°„** | **ì•½ 1ì´ˆ** |

### Response í¬ê¸°

- **Chunk #1**: ~350 bytes
- **Chunk #2**: ~180 bytes
- **ì´ í¬ê¸°**: ~530 bytes

---

## ğŸ” ë””ë²„ê¹… ì •ë³´

### API í‚¤ ì²˜ë¦¬

ë¡œê·¸ì—ì„œ API í‚¤ê°€ ì„¸ ê°€ì§€ í˜•íƒœë¡œ ì¶œë ¥ë©ë‹ˆë‹¤:

1. **ë§ˆìŠ¤í‚¹ëœ í‚¤**: `AIzaSyAjvi...6XJ8`
2. **ì „ì²´ í‚¤**: `AIzaSyAjvi-s0iKtQoFoS7yNRXJ4zZDkQqv6XJ8`
3. **í™˜ê²½ ë³€ìˆ˜ í‚¤**: í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜¨ í‚¤
4. **ì„¤ì • íŒŒì¼ í‚¤**: configì—ì„œ ì½ì–´ì˜¨ í‚¤

### ê¸´ ë‚´ìš© ìš”ì•½

ìš”ì²­ ë³¸ë¬¸ì˜ ê¸´ ë¬¸ìì—´ì€ ìë™ìœ¼ë¡œ ìš”ì•½ë©ë‹ˆë‹¤:

```
"content": "You are opencode, an interactive CLI tool... (ì´ 10070ì)"
```

### Tool ì •ë³´ ìš”ì•½

- **ì²« ë²ˆì§¸ ë„êµ¬**: ì „ì²´ ìŠ¤í‚¤ë§ˆ í‘œì‹œ
- **ë‚˜ë¨¸ì§€ ë„êµ¬**: ì´ë¦„ë§Œ í‘œì‹œ

---

## ğŸ’¡ Best Practices

### 1. ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš©

ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ `stream: true` ì‚¬ìš©:

```json
{
  "stream": true,
  "stream_options": {
    "include_usage": true
  }
}
```

### 2. ì ì ˆí•œ max_tokens ì„¤ì •

ì‘ë‹µ ê¸¸ì´ë¥¼ ì œí•œí•˜ì—¬ ë¹„ìš© ì ˆê°:

```json
{
  "max_tokens": 1000
}
```

### 3. Tool Choice ìµœì í™”

í•„ìš”í•œ ê²½ìš°ì—ë§Œ íŠ¹ì • ë„êµ¬ ê°•ì œ ì‚¬ìš©:

```json
{
  "tool_choice": "auto"  // ë˜ëŠ” {"type": "function", "function": {"name": "specific_tool"}}
}
```

### 4. ëª¨ë¸ ì„ íƒ

ìš©ë„ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ:

- **ë¹ ë¥¸ ì‘ë‹µ**: `gemini-2.5-flash`
- **ë³µì¡í•œ ì‘ì—…**: `gemini-1.5-pro`

---

## ğŸ“ˆ ì„±ëŠ¥ ì¸¡ì •

### ì‘ë‹µ ì‹œê°„ ë¶„ì„

```
ìš”ì²­ ì‹œì‘ â”€â”€â”€â”€> LiteLLM í˜¸ì¶œ â”€â”€â”€â”€> ì²« ì²­í¬ â”€â”€â”€â”€> ì™„ë£Œ
   |               |                |            |
 0ms            50ms            1000ms       1100ms
```

### í† í° ì‚¬ìš©ëŸ‰ (ì˜ˆìƒ)

- **Prompt Tokens**: ~150
- **Completion Tokens**: ~20
- **Total Tokens**: ~170

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### Python í…ŒìŠ¤íŠ¸

```python
import requests
import json

url = "http://localhost:8000/chat/completions"
headers = {"Content-Type": "application/json"}

data = {
    "model": "local-model",
    "messages": [
        {"role": "user", "content": "ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"}
    ],
    "stream": True
}

response = requests.post(url, headers=headers, json=data, stream=True)

for line in response.iter_lines():
    if line:
        line_str = line.decode('utf-8')
        if line_str.startswith('data: '):
            data_str = line_str[6:]
            if data_str != '[DONE]':
                chunk = json.loads(data_str)
                content = chunk['choices'][0]['delta'].get('content', '')
                if content:
                    print(content, end='', flush=True)
```

### cURL í…ŒìŠ¤íŠ¸

```bash
curl -X POST http://localhost:8000/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [
      {"role": "user", "content": "ë„ˆì— ëŒ€í•´ì„œ ì†Œê°œí•´ì¤˜"}
    ],
    "stream": true
  }' \
  --no-buffer
```

---

## ğŸ“ ì£¼ìš” íŠ¹ì§• ìš”ì•½

### âœ… ì„±ê³µ ì¼€ì´ìŠ¤

1. **ëª¨ë¸ ë§¤í•‘**: `local-model` â†’ `openai/gemini-2.5-flash` ì •ìƒ ë³€í™˜
2. **ìŠ¤íŠ¸ë¦¬ë°**: 2ê°œì˜ ì²­í¬ë¡œ ì •ìƒ ì‘ë‹µ
3. **Tool ì „ë‹¬**: 12ê°œì˜ ë„êµ¬ê°€ ì •ìƒì ìœ¼ë¡œ LiteLLMìœ¼ë¡œ ì „ë‹¬
4. **í•œê¸€ ì§€ì›**: UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì •ìƒ ì²˜ë¦¬

### ğŸ”§ ê°œì„  ê°€ëŠ¥ ì‚¬í•­

1. **ì‘ë‹µ ì‹œê°„**: ì¶”ê°€ ìµœì í™” ê°€ëŠ¥
2. **í† í° ì‚¬ìš©ëŸ‰**: usage ì •ë³´ í¬í•¨ ì˜µì…˜ í™œì„±í™”
3. **ì—ëŸ¬ ì²˜ë¦¬**: ë” ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
4. **ìºì‹±**: ë°˜ë³µ ìš”ì²­ì— ëŒ€í•œ ìºì‹± ê³ ë ¤

---

## ğŸ“ ë¬¸ì˜

ì‹¤ì œ ì‚¬ìš© ì¤‘ ë¬¸ì œë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
